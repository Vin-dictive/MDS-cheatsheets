\documentclass[8pt,landscape]{article}

% PACKAGES
\usepackage[letterpaper, margin=0.1in]{geometry}
\usepackage{amsmath, amssymb, geometry}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{sectsty}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{booktabs} % For better tables if needed

% Define a custom color for code highlighting and environment
\definecolor{myred}{RGB}{204, 0, 0} % A strong red
\definecolor{mygray}{RGB}{240, 240, 240} % Light gray for background

% LISTINGS SETUP for R and Python
\lstset{
  basicstyle=\ttfamily\scriptsize\color{black}, % Tiny font for code
  backgroundcolor=\color{mygray},
  frame=single,
  frameround=tttt,
  framesep=3pt,
  rulesepcolor=\color{black!20},
  breaklines=true,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue!80!black},
  stringstyle=\color{myred}, % Use red for strings to match the custom \code{} style
  breakatwhitespace=true,
  xleftmargin=0pt,
  xrightmargin=0pt,
  aboveskip=0.5ex,
  belowskip=0.5ex
}

% FORMATTING
\setstretch{0.9} % Reduce line spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

% Reduce section spacing and change color
\sectionfont{\fontsize{8}{9}\selectfont\bfseries\color{black}} % Main section font: 8pt
% Reduce subsection spacing and make subsections blue
\makeatletter
\renewcommand{\subsection}{\@startsection{subsection}{2}{0pt}%
    {0.1ex}% space before subsection
    {0.1ex}% space after subsection
    {\fontsize{8}{9}\bfseries\color{blue}}} % Subsection font: 8pt
\makeatother

% Custom command for red-highlighted code/commands (for in-line use)
\newcommand{\code}[1]{\textcolor{myred}{\texttt{#1}}}

% Custom small text wrapper - ensuring 8pt for body text
\newcommand{\smalltext}[1]{%
  {\fontsize{8}{9}\selectfont\sloppy #1\par}%
}

% DOCUMENT START
\begin{document}
\fontsize{8}{9}\selectfont % Set the base font size to 8pt and line skip to 9pt for the entire document
\pagestyle{empty}
\begin{multicols}{3}

\section{Core ML Concepts}

\subsection{Basic Terminology}
\smalltext{
\textbf{Features ($\mathbf{X}$):} Relevant characteristics of the problem, usually suggested by experts. $d$ is the number of features. \\
\textbf{Target ($\mathbf{y}$):} The feature we want to predict. \\
\textbf{Example:} A row of feature values (may or may not include $y$). $n$ is the number of examples. \\
\textbf{Training:} The process of learning the mapping between $X$ and $y$. \\
}

\subsection{AI, ML, and DL}
\smalltext{
\textbf{Artificial Intelligence (AI):} Broad goal of making computers perform tasks that typically require human intelligence. \\
\textbf{Machine Learning (ML):} A subset of AI where systems learn patterns from data instead of being explicitly programmed. \\
\textbf{Deep Learning (DL):} A subset of ML that uses multi-layered neural networks to learn complex patterns.
}

\section{Scikit-learn Workflow}

\subsection{Classification Steps}
\smalltext{
\begin{enumerate}[label=\arabic*., nosep, itemsep=0pt, topsep=0pt, partopsep=0pt]
    \item Read the data
    \item Create $X$ and $y$ (features and target)
    \item Create a classifier object
    \item \code{fit} the classifier
    \item \code{predict} on new examples
    \item \code{score} the model
\end{enumerate}
}

\subsection{Parameters vs. Hyperparameters}
\smalltext{
\textbf{Parameters:} Values learned by the algorithm from the data during \code{fit} (e.g., split features/thresholds in a decision tree). \\
\textbf{Hyperparameters:} "Knobs" set *before* calling \code{fit} that control the learning (e.g., \code{max\_depth} in a decision tree). Set via expert knowledge, heuristics, or optimization.
}

\section{Decision Trees}

\subsection{Tree Terminology}
\smalltext{
\textbf{Root Node:} The first condition/question to check. \\
\textbf{Branch:} Connects nodes, typically representing true or false. \\
\textbf{Internal Node:} Represents conditions within the tree. \\
\textbf{Leaf Node:} Represents the predicted class/value. \\
\textbf{Tree Depth:} The number of edges on the path from the root node to the farthest leaf node.
}

\subsection{Regression Trees}
\smalltext{
\textbf{Model:} Use \code{DecisionTreeRegressor}. \\
\textbf{Paradigm:} \code{fit} and \code{predict} are similar to classification. \\
\textbf{Score:} The \code{score} method returns the $\mathbf{R^2}$ \textbf{score}. \\
\textbf{$\mathbf{R^2}$:} Maximum is 1 (perfect predictions). Can be negative (worse than \code{DummyRegressor} which returns the mean of $y$). \\
\textbf{Common Metric:} Mean Squared Error (MSE).
}
\begin{lstlisting}[language=Python]
X = regression_df.drop(["quiz2"], axis=1)
y = regression_df["quiz2"]

depth = 2
reg_model = DecisionTreeRegressor(max_depth=depth)
reg_model.fit(X, y)
regression_df["predicted_quiz2"] = reg_model.predict(X)
print("R^2 score: %0.3f" % (reg_model.score(X, y)))
\end{lstlisting}

\subsection{Classification Example}
\begin{lstlisting}[language=Python]
depth = 2
model = DecisionTreeClassifier(max_depth=depth)
model.fit(X_subset.values, y)
# Score on training data
score = model.score(X_subset.values, y)
print("Error: %0.3f" % (1 - score))
\end{lstlisting}

\section{Model Evaluation}

\subsection{Train, Validation, Test Split}
\smalltext{
\textbf{Golden Rule:} The test data cannot influence the training phase in any way. \\
\textbf{Deployment Data:} Data in the wild where $y$ is unknown. Deployment error is the true goal. \\
\textbf{Validation Data (Dev Set):} Used for hyperparameter tuning. Locked in a "vault" until ready to evaluate. \\
\textbf{Expected Error Hierarchy:} \\
$E\text{-train} < E\text{-validation} < E\text{-test} < E\text{-deployment}$
}
\begin{lstlisting}[language=Python]
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(
    df, test_size=0.2, random_state=123
) 
X_train, y_train = train_df.drop(columns=["country"]), train_df["country"]
X_test, y_test = test_df.drop(columns=["country"]), test_df["country"]
\end{lstlisting}

\subsection{Cross-Validation (CV)}
\smalltext{
\textbf{Purpose:} Evaluate how well the model generalizes to unseen data (gets validation scores). \\
\textbf{Process:} Split data into $k$ folds (e.g., $k=10$). Each fold gets a turn as the validation set. CV does \textit{not} shuffle data (that's \code{train\_test\_split}). \\
\textbf{\code{cross\_validate}:} More powerful than \code{cross\_val\_score}. Gives access to training and validation scores.
}
\begin{lstlisting}[language=Python]
from sklearn.model_selection import cross_val_score, cross_validate
model = DecisionTreeClassifier(max_depth=4)
cv_scores = cross_val_score(model, X_train, y_train, cv=10)
# Use cross_validate to get both train and test (validation) scores
scores = cross_validate(model, X_train, y_train, cv=10, 
                        return_train_score=True)
\end{lstlisting}

\section{Bias-Variance Tradeoff}

\subsection{Underfitting (High Bias)}
\smalltext{
\textbf{Cause:} Model is too simple (\code{max\_depth=1}). Fails to capture useful training patterns. \\
\textbf{Effect:} Both train and validation errors are \textbf{high}. Low gap between them. \\
\textbf{Error Relation:} $E\text{-best} < E\text{-train} \le E\text{-valid}$
}

\subsection{Overfitting (High Variance)}
\smalltext{
\textbf{Cause:} Model is too complex (\code{max\_depth=None}). Learns unreliable, noisy training patterns. \\
\textbf{Effect:} Training error is very \textbf{low}, but there is a \textbf{big gap} between training and validation error. \\
\textbf{Error Relation:} $E\text{-train} < E\text{-best} < E\text{-valid}$ \\
\textbf{Tradeoff:} As complexity $\uparrow$, $E\text{-train} \downarrow$ but $(E\text{-valid} - E\text{-train}) \uparrow$. We want to avoid both.
}

\section{Preprocessing}

\subsection{Preprocessing}
\smalltext{
\textbf{Imputation:} Tackling missing values. \\
\textbf{Scaling:} Scaling of numeric features (e.g., MinMax or Standardization). \\
\textbf{One-hot encoding:} Tackling categorical variables.
}
\begin{lstlisting}[language=Python]
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer # For handling missing values
# Define feature types (X_train.columns assumed)
numeric_features = ['age', 'income']
categorical_features = ['city', 'gender']
# Create transformers for different column types
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
# Create the preprocessor using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])
# Fit and transform the data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
\end{lstlisting}

\subsection{Euclidean Distance}
\smalltext{
Used to find the distance between 2 feature vectors (a row).
}
\begin{lstlisting}[language=Python]
# Euclidean distance using sklearn
from sklearn.metrics.pairwise import euclidean_distances

euclidean_distances(two_cities)
\end{lstlisting}

\subsection{$\text{k-NN}$ KNeighborsClassifier Hyperparameter Tuning}
\smalltext{
Example loop to tune $k$ (\code{n\_neighbors}) using cross-validation and collect scores.
}
\begin{lstlisting}[language=Python]
results_dict = {...} # See full data
param_grid = {"n_neighbors": np.arange(1, 50, 5)}
for k in param_grid["n_neighbors"]:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_validate(knn, X_train, y_train, 
                            return_train_score=True)
    results_dict["n_neighbors"].append(k)
    results_dict["mean_cv_score"].append(np.mean(scores["test_score"]))
    results_dict["mean_train_score"].append(np.mean(scores["train_score"]))
    # ... append standard deviations ...
results_df = pd.DataFrame(results_dict)
best_n_neighbours = results_df.idxmax()["mean_cv_score"]
knn = KNeighborsClassifier(n_neighbors=best_n_neighbours)
knn.fit(X_train, y_train)
print("Test accuracy: %0.3f" % (knn.score(X_test, y_test)))
\end{lstlisting}


\subsection{$\text{k-NN}$ KNeighborsRegressor Hyperparameter Tuning}
\smalltext{
Example loop to tune $k$ (\code{n\_neighbors}) using cross-validation and collect scores.
}
\subsection{Pros of k-NNs for supervised learning}
\smalltext{
Easy to understand, interpret. \\
Simple hyperparameter \(n_neighbors\) controlling the fundamental tradeoff. \\
Can learn very complex functions given enough data. \\
Lazy learning: Takes no time to fit \\
}
\subsection{Cons of k-NNs for supervised learning}
\smalltext{
Can be potentially be VERY slow during prediction time, especially when the training set is very large.\\
Often not that great test accuracy compared to the modern approaches. \\
It does not work well on datasets with many features or where most feature values are 0 most of the time (sparse datasets).
}
\subsection{Curse of dimensionality}
\smalltext{
Affects all learners but especially bad for nearest-neighbour. \\
k-NN usually works well when the number of dimensions d is small but things fall apart quickly as d goes up. \\
If there are many irrelevant attributes, k-NN is hopelessly confused because all of them contribute to finding similarity between examples. \\ d
With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and k-NN is no better than random guessing.
}
\subsection{Support Vector Machines (SVMs) with RBF kernel}
\smalltext{
Affects all learners but especially bad for nearest-neighbour. \\
k-NN usually works well when the number of dimensions d is small but things fall apart quickly as d goes up. \\
If there are many irrelevant attributes, k-NN is hopelessly confused because all of them contribute to finding similarity between examples. \\ d
With enough irrelevant attributes the accidental similarity swamps out meaningful similarity and k-NN is no better than random guessing.
}



\end{multicols}
\end{document}