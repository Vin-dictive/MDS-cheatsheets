\documentclass[8pt,landscape]{article}

% PACKAGES
\usepackage[letterpaper, margin=0.1in]{geometry}
\usepackage{amsmath, amssymb, geometry}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{sectsty}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{booktabs} % For better tables if needed

% Define a custom color for code highlighting and environment
\definecolor{myred}{RGB}{204, 0, 0} % A strong red
\definecolor{mygray}{RGB}{240, 240, 240} % Light gray for background
\setlist[itemize]{
    noitemsep,  % Removes vertical space between list items
    topsep=0pt,  % Removes space before the list
    partopsep=0pt, % Removes extra space when a list begins mid-paragraph
    parsep=0pt, % Removes space between paragraphs within an item
    leftmargin=*, % Sets the left margin to the minimum necessary
    labelsep=0.5em, % You can slightly adjust the space between the bullet and the text
}
% LISTINGS SETUP for R and Python
\lstset{
  basicstyle=\ttfamily\scriptsize\color{black},
  backgroundcolor=\color{mygray},
  frame=single,
  frameround=tttt,
  framesep=0pt,        % no padding between code and frame
  rulecolor=\color{black!20},
  rulesep=0pt,         % no space between rules
  breaklines=true,
  columns=fullflexible,
  showstringspaces=false,
  commentstyle=\color{gray},
  keywordstyle=\color{blue!80!black},
  stringstyle=\color{myred},
  breakatwhitespace=true,
  xleftmargin=0pt,     % no left margin
  xrightmargin=0pt,    % no right margin
  aboveskip=0pt,       % no space above listing
  belowskip=0pt,       % no space below listing
  abovecaptionskip=0pt,
  belowcaptionskip=0pt,
  framexleftmargin=0pt,
  framexrightmargin=0pt,
  framextopmargin=0pt,
  framexbottommargin=0pt
}

% FORMATTING
\setstretch{0.9} % Reduce line spacing
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

% Reduce section spacing and change color
\makeatletter
\renewcommand{\section}{\@startsection{section}{2}{0pt}%
    {0.1ex}% space before section
    {0.1ex}% space after section
    {\fontsize{8}{9}\bfseries\color{red}}} % section font: 8pt
\makeatother

% Reduce subsection spacing and make subsections blue
\makeatletter
\renewcommand{\subsection}{\@startsection{subsection}{2}{0pt}%
    {0.1ex}% space before subsection
    {0.1ex}% space after subsection
    {\fontsize{8}{9}\bfseries\color{blue}}} % Subsection font: 8pt
\makeatother

% Custom command for red-highlighted code/commands (for in-line use)
\newcommand{\code}[1]{\textcolor{myred}{\texttt{#1}}}

% Custom small text wrapper - ensuring 8pt for body text
\newcommand{\smalltext}[1]{
  {\fontsize{8}{9}\selectfont\sloppy #1\par}
}

% DOCUMENT START
\begin{document}
\fontsize{8}{9}\selectfont % Set the base font size to 8pt and line skip to 9pt for the entire document
\pagestyle{empty}
\begin{multicols}{3}

% \section{Ordinary Least-squares Regression}
% \subsection{Assumptions}
% \smalltext{
%     Response=Systematic Component+Random Component.
%     .$\epsilon$  is the Random Component under the following assumptions:
%     .each Yi is also assumed to be independent and normally distributed
%     .qq plot lying on the 45° degree dotted line. Standard Normal distribution
%     .Histogram of residuals bell-shaped form as in the Normal distribution.
%     .Homoscedasticity can be assessed via the diagnostic plot of residuals vs. fitted values. Funnel shapes indicates non-constant variance, i.e., heteroscedasticity.
%     .This assumption commonly gets violated in multiple linear regression and is called heteroscedasticity: the variance of the $\epsilon_i$s is not constant.
%     .OLS not suffice - Non-negative values.  and Binary outcomes (Success or Failure). and Count data. 
% }
\subsection{GLM - Nature of the Model Function}
\smalltext{
    \textbf{Deterministic:}For each one of the values of the regressor X, there is a single value of Y.
    \textbf{Stochastic:} Each value of X has a probability distribution associated to Y.
    \textbf{Black-box Models:}is focused on optimizing predictions subject to a set of regressors with less attention on the internal model's process. 
    \textbf{Link function:}OLS regression models a continuous response $Y_i$  (a random variable) via its conditioned mean (or expected value) $\mu_i$ subject to $k$  regressors $X_{i,j}$. modelling the mean $\mu_i$ of a discrete-type response (such as binary or a count) is not straightforward. 
    Hence, we rely on a monotonic and differentiable function called the link function.
}

\section{Poisson Regression}
\smalltext{
        GLM to model count-type responses.
        Bar chart count is right skewed then poisson.
        The equality of the expected value and variance in a random variable is called equidispersion.
}
\smalltext{
The estimates are obtained through maximum likelihood where we assume a Poisson joint probability mass function of the n responses Yi.}
\begin{lstlisting}[language=R]
library(glmbb)
data(crabs)
crabs <- crabs |>  rename(n_males = satell) |>  dplyr::select(-y)
group_avg_width <- crabs |> mutate(intervals = cut(crabs$width, breaks = 10)) |> group_by(intervals) |> summarise(mean = mean(n_males), n = n()) 
poi_model <- glm(n_males ~ width, family = poisson, data = crabs)
\end{lstlisting}

\smalltext{\textbf{Inference:}The fitted regression model will be used to identify the relationship between the logarithm of the response’s mean and regressors. To determine the statistical significance of 
 in this model, we also use the Wald statistic.}
\begin{lstlisting}[language=R]
tidy(poi_model, conf.int = TRUE) |> mutate_if(is.numeric, round, 3)
\end{lstlisting}
\smalltext{
    Our sample gives us evidence to reject $H\_0$ ($p-value < 0.001$). 
    So carapace width is statistically associated to the logarithm of the mean of n\_males.
    }

\subsection{Coefficient Interpretation}
\smalltext{
Moreover, it has a baseline: dark. We can check the baseline level, via levels().
}
\begin{lstlisting}[language=R]
poi_model_2 <- glm(n_males~width+color, family = poisson, data =)
tidy(poi_model_2, exponentiate = TRUE, conf.int = TRUE)
\end{lstlisting}
\smalltext{
1.55 indicates that the mean count of male crabs (n\_males) around a female breeding nest increases by 55\%
 when the female color of the prosoma changes from dark to light, while keeping the carapace female width constant.
}
\subsection{Predictions}
\begin{lstlisting}[language=R]
round(predict(poisson_model_2, newdata = tibble(width = 27.5, color = "light"), type = "response"), 2)
\end{lstlisting}

\section{Overdispersion}
\smalltext{
When the variance is larger than the mean in a random variable, we have overdispersion. This matter will impact the standard error of our parameter estimates in a basic Poisson regression, as we will see further.
}
with the hypotheses
$
H_{0} : 1 + \gamma = 1 |   H_{a} : 1 + \gamma > 1.
$
When there is evidence of overdispersion in our data, \textbf{we will reject $H_{0}$}.
\begin{lstlisting}[language=R]
dispersiontest(poisson_model_2) {AER}
\end{lstlisting}
\smalltext{
    With $\alpha = 0.05$, we reject $H\_{0}$ since the $p\text{-value} < .001$. Hence, the \texttt{poisson\_model\_2} has overdispersion.
    Using a Negative Binomial model when overdispersion is present helps control your Type I error rate. Hence lower standard error.
}

\section{Negative Binomial Regression}
\smalltext{
    A Negative Binomial random variable depicts the number of $y_i$
 failed independent Bernoulli trials before experiencing $m$
 successes with a probability of success $p_i$
}
\smalltext{The estimates are obtained through maximum likelihood where we assume a Poisson joint probability mass function of the n responses Yi.}
\begin{lstlisting}[language=R]
library(MASS)
negative_bin_model <- glm.nb(n_males ~ width + color, data = crabs)
summary(negative_bin_model)
\end{lstlisting}

\section{Model Selection}
\begin{lstlisting}[language=R]
poi_model<-glm(n_males~ width,family = poisson,data = crabs)
poi_model_2<-glm(n_males~width+color,family = poisson,data = crabs)
library(broom)
summary_poisson_model_2 <- glance(poisson_model_2)
\end{lstlisting}
\smalltext{
    We can compare the fits provided by these two models by the deviance. $D_k = kmodel/fullModel$ is formally called residual deviance, which is the test statistic.
    Large $D_k$ means model fits poorly compared to the baseline model.
    Small $D_k$ means model fits good  compared to the baseline model.
    We cannot use anova() to perform this hypothesis testing. We will have to do it manually via glance().
}
\begin{lstlisting}[language=R]
pchisq(summary_poisson_model_2$deviance, #p-value for this test
  df = summary_poisson_model_2$df.residual,  lower.tail = FALSE) 
\end{lstlisting}
\smalltext{
    In Poisson regression, the pchisq test on your model's deviance is a Goodness-of-Fit (GoF) test.
    $p$-value $\le .001$:Reject $H_0$: strong evidence that your model does not fit the data.
}

\subsection{Analysis of Deviance for Nested Models}
\begin{lstlisting}[language=R]
round(anova(poisson_model, poisson_model_2, test = "Chi"), 4)
\end{lstlisting}
\smalltext{
$
H_{0} : \text{The simpler model (Model 1) is sufficient.} \\
H_{a} : \text{The more complex model (Model 2) fits significantly better.}
$
With a $p$-value $\le 0.05$ (Pr(\>Chi)), we reject $H_{0}$. Significant evidence that \texttt{poisson\_model\_2} fits the data better than the simpler \texttt{poisson\_model}. Therefore, we select \texttt{poisson\_model\_2}, which includes the additional predictor for the color of the prosoma.
}

\subsection{Akaike Information Criterion-glance()}
\smalltext{
Drawbacks of the analysis of deviance - tests only nested regression models.
AIC makes possible to compare models that are either nested or not.
$AIC_k$ favours models with small values of $D_k$. Models with smaller values of $AIC_k$ are preferred because 
$AIC_k = D_k+2k$. It also penalizes for including more regressors in the model. Hence, it discourages overfitting. 
}

\subsection{Bayesian Information Criterion-glance()}
\smalltext{
    An alternative to AIC. The BIC also makes possible to compare models that are either nested or not. For a model with $k$ regressors, $n$ observations used for training, and a deviance $D_k$
; it is defined as: $BIC_k = D_k+k * Log(n)$. Models with smaller values of BIC are preferred.
}


\section{Multinomial Logistic Regression}
\smalltext{
    Categorical Type Responses - more than two classes in the categorical response.
    Models the logarithm of odds.}

\begin{lstlisting}[language=R]
log_model <- glm(formula = genre ~ danceability + valence, data = data, family = binomial)
library(broom) tidy(log_model, conf.int = TRUE, exponentiate = TRUE)
\end{lstlisting}

\smalltext{To fit the model with the package nnet, we use the function multinom(), which obtains the corresponding estimates. Final output is converted those scores into class probabilities using softmax functions}
\smalltext{\textbf{Inference:} Provided the sample size n is large enough, it has an approximately Standard Normal distribution under $H_o$.}

\begin{lstlisting}[language=R]
model <- multinom(formula = genre ~ danceability + valence, data = spotify_training)
mult_output <- tidy(model, conf.int = TRUE, exponentiate = TRUE) |>  dplyr::filter(p.value < 0.05)    
\end{lstlisting}
\smalltext{
Baseline response is edm, we can conclude
There is a statistical difference in danceability in edm versus r\&b and rock.
}
\subsection{Coefficient Interpretation}
\smalltext{
$\beta_1^{(latin,edm)}$:for each unit increase in the valence score in the Spotify catalogue, the song is 1.05 times more probable to be latin than edm.
}

\subsection{Predictions}
\begin{lstlisting}[language=R]
pred_probs <- round(predict(model, tibble(danceability = 27.5, valence = 30), type = "probs"), 2)
\end{lstlisting}

\section{Ordinal Logistic Regression}
\begin{lstlisting}[language=R]
college_data$decision <- as.ordered(college_data$decision)
college_data$decision <- fct_relevel(college_data$decision, c("unlikely", "somewhat likely", "very likely"))
levels(college_data$decision)
\end{lstlisting}
\smalltext{
The categories 1,2,3,4,5 in the i-th response $Y_i$ implicate an ordinal scale.
Hence, our Ordinal Logistic regression model will indicate how each one of the 4 regressors that affects the cumulative logarithm of the odds in the ordinal response.
we would need four link functions, four intercepts, and four coefficients.
}
\begin{lstlisting}[language=R]
ordinal_model <- polr(decision ~ parent_ed + GPA, data = data, Hess = TRUE)

library(broom)
summary_ordinal_model <- cbind(tidy(ordinal_model), p.value = pnorm(abs(tidy(ordinal_model)$statistic), lower.tail = FALSE) * 2) |>  mutate_if(is.numeric, round, 2)
round(confint(ordinal_model), 2)
tibble(summary_ordinal_model[1:2, 1:2], exp.estimate = round(exp(summary_ordinal_model[1:2, 2]), 2))
\end{lstlisting}
\smalltext{
  \textbf{Inference:}
  $\beta_1$ : “for each one-unit increase in the GPA, the odds that the student is very likely versus somewhat likely or unlikely to apply to graduate school increase by \_\_ times (while holding parent\_ed constant).”
  $\beta_2$ : “for those respondents whose parents attended to graduate school, the odds that the student is very likely versus somewhat likely or unlikely to apply to graduate school increase by exp($\beta_2$)=2.86 times.
}
\subsection{Brant-Wald Test}
\smalltext{
    OLM under the proportional odds assumption is the first step when performing Regression Analysis on an ordinal response.
    Is it possible to assess whether it fulfils this strong assumption statistically.
}
\begin{lstlisting}[language=R]
library(brant) brant(ordinal_model)
\end{lstlisting}
\smalltext{
row Omnibus represents the global model. Note that with $\alpha=0.05$, we are completely fulfilling the proportional odds assumption.
Suppose that does not fulfil the proportional odds assumption, we can model under a non-proportional odds assumption. This is called a Generalized OLR model. 
}

\section{Linear Mixed-effects Models}
\smalltext{
    A panel refers to a dataset in which each individual (e.g., a firm) is observed within a timeframe. Furthermore, the term balanced indicates that we have the same number of observations per individual.
}
\subsection{OLS Regression with Varying Intercept}
\smalltext{
We will do this with the lm() function by adding - 1 on the right-hand side of the argument formula. This - 1 will allow the baseline firm to have its intercept (i.e., replacing the usual (Intercept) in column estimate with this specific baseline company’s intercept). In this case, General Motors is the baseline company (as it appears on the left-had side of the levels() output).
}
\begin{lstlisting}[language=R]
model_varying_intercept <- lm(formula = investment ~ market_value + capital + firm - 1,data = Grunfeld)
tidy(model_varying_intercept) glance(model_varying_intercept)
\end{lstlisting}
\smalltext{
    By checking the adj.r.squared, we see that model\_varying\_intercept has a larger value (0.959) than ordinary\_model (0.816) (i.e., the first fitted model without firm as a regressor). 
}
\smalltext{
   $
\text{investment}_{i,j} = \beta_{0,j} + \beta_{1,j}\text{marketValue}_{i,j} + \beta_{2,j}\text{capital}_{i,j} + \varepsilon_{i,j} \\
\text{for } i = 1, \dots, 20 \text{ and } j = 1, \dots, 11.
$
}
\begin{lstlisting}[language=R]
anova(ordinary_model, model_varying_intercept)
\end{lstlisting}
\smalltext{
    With $pvalue \le \alpha=0.05$, we have evidence to conclude that model\_varying\_intercept fits the data better than the ordinary\_model.
    However, this costs us one extra degree of freedom per firm except for the baseline. Therefore, we lose another 10 degrees of freedom (column DF in the anova() output).
}

\subsection{OLS Regression for Each Category}
\smalltext{
    We can make the model more complex with two interactions (market\_value * firm and capital * firm). This will estimate a linear regression by firm with its own slopes.
}
\begin{lstlisting}[language=R]
model_by_firm <- lm(investment ~ market_value * firm + capital * firm, data = Grunfeld)
tidy(model_by_firm) glance(model_by_firm) 
\end{lstlisting}
\smalltext{
    $
    \text{investment}_{i,j} = \beta_{0,j} + \beta_{1,j}\text{marketValue}_{i,j} + \beta_{2,j}\text{capital}_{i,j} + \varepsilon_{i,j} \\
    \text{for } i = 1, \dots, 20 \text{ and } j = 1, \dots, 11.
$
}
\smalltext{
  \textbf{Inference:}
    Each regression coefficient is associated with a \texttt{firm}. For example, \texttt{firmUS Steel:capital} $= 0.02$ means that the variable \texttt{capital} has a slope of $0.02 + 0.37 = 0.39$ for \texttt{US Steel}. We can double-check this by estimating an individual linear regression for \texttt{US Steel}:
}
\begin{lstlisting}[language=R]
tidy(lm(investment ~ market_value + capital,
  data = Grunfeld |> filter(firm == "US Steel")
)) |>  mutate_if(is.numeric, round, 2) |> print(n = Inf)
\end{lstlisting}

\subsection{Full Mixed Models}
\begin{lstlisting}[language=R]
full_mixed_model <- lmer(  investment ~ market_value +  capital + (market_value + capital | firm), data = Grunfeld)
library(lmerTest)
summary(mixed_intercept_model)
coef(mixed_intercept_model)$firm
round(predict(full_mixed_model, newdata = tibble(firm = "General Motors",market_value = 2000, capital = 1000)), 2)
\end{lstlisting}
\end{multicols}
\end{document}
